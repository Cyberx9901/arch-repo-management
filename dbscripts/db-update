#!/usr/bin/python

import asyncio
import json
from asyncio.subprocess import PIPE
from base64 import b64encode
from fcntl import LOCK_EX, flock
from hashlib import md5, sha256
from pathlib import Path
from sys import argv, exit

from lib.dbwrite import generate_dbs
from pyalpm import vercmp


def parse_pkginfo(pkginfo: str) -> dict:
    fields = {}
    for line in pkginfo.splitlines():
        line = line.strip()
        if line.startswith("#"):
            continue
        key, value = line.split(" = ", 1)
        fields.setdefault(key, []).append(value)
    return fields


def parse_pkgfiles(pkginfo: str) -> dict:
    files = []
    for line in pkginfo.splitlines():
        line = line.strip()
        if not line.startswith("."):
            files.append(line)
    return sorted(files)


def parse_gpgstatus(status: str) -> list:
    return [line.split() for line in status.splitlines()]


async def run(*args):
    args = [str(a) for a in args]
    proc = await asyncio.create_subprocess_exec(*args)
    if await proc.wait() != 0:
        raise RuntimeError(f"Command failed: {args!r}")


async def get_output(*args) -> str:
    args = [str(a) for a in args]
    proc = await asyncio.create_subprocess_exec(*args, stdout=PIPE)
    stdout, _ = await proc.communicate()
    if proc.returncode != 0:
        raise RuntimeError(f"Command failed: {args!r}")
    return stdout.decode().strip()


async def put_input(*args, stdin: str):
    args = [str(a) for a in args]
    proc = await asyncio.create_subprocess_exec(*args, stdin=PIPE)
    await proc.communicate(stdin.encode())
    if proc.returncode != 0:
        raise RuntimeError(f"Command failed: {args!r}")


async def get_pkginfo(pkgfile: Path) -> dict:
    return parse_pkginfo(await get_output("bsdtar", "-xOf", pkgfile, ".PKGINFO"))


async def get_pkgfiles(pkgfile: Path) -> dict:
    return parse_pkgfiles(await get_output("bsdtar", "-tf", pkgfile))


async def verify_gpg(pkgfile: Path):
    # verify PGP signature
    return parse_gpgstatus(
        await get_output(
            "gpg",
            "--quiet",
            "--logger-file=/dev/null",
            "--status-fd=1",
            "--batch",
            "--no-tty",
            "--verify",
            pkgfile.parent / f"{pkgfile.name}.sig",
            pkgfile,
        )
    )


def build_pkgmeta(pkgpath, pkginfo, pkgfiles):
    hash_md5 = md5()
    hash_sha256 = sha256()
    with pkgpath.open(mode="rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
            hash_sha256.update(chunk)

    with (pkgpath.parent / f"{pkgpath.name}.sig").open(mode="rb") as f:
        pgpsig = b64encode(f.read()).decode()

    meta = {
        "files": pkgfiles,
        # Calculated
        "filename": pkgpath.name,
        "csize": pkgpath.stat().st_size,
        "md5sum": hash_md5.hexdigest(),
        "sha256sum": hash_sha256.hexdigest(),
        "pgpsig": pgpsig,
        # Must-have fields
        "name": pkginfo["pkgname"][0],
        "desc": pkginfo["pkgdesc"][0],
        "url": pkginfo["url"][0],
        "builddate": int(pkginfo["builddate"][0]),
        "packager": pkginfo["packager"][0],
        "isize": int(pkginfo["size"][0]),
        "arch": pkginfo["arch"][0],
    }

    # Arrays (may be empty)
    def maybe(meta_field, info_field):
        value = pkginfo.get(info_field)
        if value:
            meta[meta_field] = value

    maybe("licenses", "license")
    maybe("replaces", "replaces")
    maybe("groups", "group")
    maybe("conflicts", "conflict")
    maybe("provides", "provides")
    maybe("backup", "backup")
    maybe("depends", "depend")
    maybe("optdepends", "optdepend")

    return meta


def build_repometa(packages):
    repo = {}

    for pkgpath, (pkginfo, pkgfiles) in packages.items():
        try:
            pkgbase = pkginfo["pkgbase"][0]
        except KeyError:
            pkgbase = pkginfo["pkgname"][0]

        pkgver = pkginfo["pkgver"][0]
        makedepends = pkginfo.get("makedepend")
        checkdepends = pkginfo.get("checkdepend")

        baseinfo = repo.get(pkgbase)
        if baseinfo is None:
            baseinfo = repo[pkgbase] = {"version": pkgver, "packages": []}

            if makedepends:
                baseinfo["makedepends"] = makedepends
            if checkdepends:
                baseinfo["checkdepends"] = checkdepends
        else:
            # validate that common fields of every pkg have
            # the same values within the same pkgbase
            if baseinfo["version"] != pkgver:
                raise RuntimeError(f"pkgvers differ for pkgbase='{pkgbase}'")
            if baseinfo.get("makedepends") != makedepends:
                raise RuntimeError(f"makedepends differ for pkgbase='{pkgbase}'")
            if baseinfo.get("checkdepends") != checkdepends:
                raise RuntimeError(f"checkdepends differ for pkgbase='{pkgbase}'")

        # transform pkg metadata
        pkgmeta = build_pkgmeta(pkgpath, pkginfo, pkgfiles)
        baseinfo["packages"].append(pkgmeta)

    return repo


async def main() -> int:
    metadir = (Path(argv[0]).parent / "meta").resolve(strict=True)
    stagingdir = (Path(argv[0]).parent / "staging").resolve(strict=True)

    # FIXME: Put this into metadir/.git/
    lockfile = (metadir / "dbscripts.lock").open(mode="w")
    flock(lockfile, LOCK_EX)

    # find pkg files to add/update
    packages = {
        r.name: {p: None for p in r.glob("*.pkg.tar.xz")} for r in stagingdir.glob("*")
    }
    packages = {r: ps for r, ps in packages.items() if ps}
    if not packages:
        return

    # load packages in parallel
    async def load(ps, p):
        pkginfo, pkgfiles, _ = await asyncio.gather(
            get_pkginfo(p), get_pkgfiles(p), verify_gpg(p)
        )
        ps[p] = (pkginfo, pkgfiles)

    await asyncio.gather(
        *(load(ps, p) for r, ps in packages.items() for p in ps.keys())
    )

    # parse new packages into repo metadata
    pkgbases = {repo: build_repometa(ps) for repo, ps in sorted(packages.items())}

    # load existing repo metadata
    def json_load(path):
        with path.open() as f:
            return json.load(f)

    def repo_load(repo):
        repodir = metadir / repo
        return {p.stem: json_load(repodir / p) for p in repodir.glob("*.json")}

    meta = {repo: repo_load(repo) for repo in pkgbases.keys()}

    # validate new packages against existing repo
    for repo, ps in pkgbases.items():
        metarepo = meta[repo]
        for pkgbase, baseinfo in ps.items():
            curbaseinfo = metarepo.get(pkgbase)
            if curbaseinfo is None:
                continue

            # verify version is increasing
            curver = curbaseinfo["version"]
            newver = baseinfo["version"]
            if vercmp(newver, curver) < 1:
                raise RuntimeError(
                    f"Cannot update package '{pkgbase}' from version '{curver}'"
                    + f" to '{newver}', version is not increased"
                )

    # save meta info to json files and update `meta` object
    for repo, ps in pkgbases.items():
        Path(metadir / repo).mkdir(exist_ok=True)
        for pkgbase, pkgs in ps.items():
            metafile = metadir / repo / f"{pkgbase}.json"
            with metafile.open(mode="w", encoding="utf-8") as f:
                json.dump(pkgs, f, ensure_ascii=False, indent=4, sort_keys=True)
        meta[repo].update(ps)

    # rebuild DB file using `meta` object
    generate_dbs(meta)

    return 0


if __name__ == "__main__":
    exit(asyncio.run(main()))
